# Statistical inference {#intro}

These notes are based on "Regression and other stories" by @gelman2020regression. Given the different background in statistics that we potentially have, this notes aim to have a standardized tool for tutorials. Finally, these notes aim to cover the same content as weekly lectures.


Statistical inference involves math operations that result in estimates (and its uncertainties) about paramaters of some underlying process. We can think it as a learning process; we learn from data (incomplete or imperfect) with the objective of informing the parameters under a probability model.

*Questions*

1. What type of probability model is involved in linear regression? 
2. What about Fisher's exact test?
3. What about nonparametric tests (Mann-Whitney etc.)?

Regression, which is the main topic in this course, can be seen as a **measurement error model**. Under these models, we are interested in learning about some underlying pattern. One simple example is linear regression with one predictor with data measured with error, i.e., $y_i=a+bx_i+\epsilon_i$.

## Sampling distribution

**Sampling distribution** may be considered as the distribution of the statistic for all possible samples from the same population of a given sample size.

- Random sampling. Simple random sample of size $n$ from a population of size $N$.
- Measurement error model. Observations $y_i$ generated from the model $y_i=a+bx_i+\epsilon_i$ with a prespecified distribution for the errors, e.g., $\epsilon_i \sim \mathcal{N}(0,\sigma)$

Most of the time the sampling distribution will not be known, therefore we can only **estimate** it. Let's see this using the following example using the previous measurment error model $y_i=a+bx_i+\epsilon_i$. We set up

$$
a=3,\space b=2,\space \epsilon_i \sim \mathcal{N}(0,1),\space x_i\sim\mathcal{N}(0,2); \space i=1,...,100
$$

```{r warning=FALSE,message=FALSE}
a=3
b=2
n=100
epsilon=rnorm(n,0,1)
x=rnorm(n,0,2)
y=a+b*x+epsilon
```

Now let's fit a linear model to our fake data. Table \@ref(tab:CI) shows the 95% CI for the fitted slope and intercept.

```{r warning=FALSE,message=FALSE}
model=lm(y~x)
summary(model)
```

```{r CI,echo=FALSE}
kable(confint(model), caption = '95% CI for intercept and slope',
  booktabs = TRUE)
```

Some authors say that the sampling distribution is a **generative model** because it represents a random process wich could potentially generate new data.

## Estimates and standard erors

**Parameters** are unknown numbers that determine a statistical model. As mentioned before, we learn from data to construct **estimates** of parameters.

*Question*

1. What are the parameters in the measurement error model used previously?

The **standard error** is the standard deviation of an estimate. It gives us an idea of the uncertainty sorrounding the quantity of interest. As sample size increases, the standard error decreases, i.e.,

$$
SE=\frac{\sigma}{n} \implies \lim_{n\to\infty}\frac{\sigma}{n}=0
$$

