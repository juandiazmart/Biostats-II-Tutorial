[["index.html", "Tutorial notes for Biostatistics II Chapter 1 Introduction", " Tutorial notes for Biostatistics II Juan Pablo Diaz Martinez 2021-01-17 Chapter 1 Introduction These notes are based on “Regression and other stories” by Gelman, Hill, and Vehtari (2020). Given the different background in statistics that we potentially have, this notes aim to have a standardized tool for tutorials. Finally, these notes aim to cover the same content as weekly lectures. References "],["intro.html", "Chapter 2 Statistical inference 2.1 Sampling distribution 2.2 Estimates and standard erors", " Chapter 2 Statistical inference Statistical inference involves math operations that result in estimates (and its uncertainties) about paramaters of some underlying process. We can think it as a learning process; we learn from data (incomplete or imperfect) with the objective of informing the parameters under a probability model. Questions What type of probability model is involved in linear regression? What about Fisher’s exact test? What about nonparametric tests (Mann-Whitney etc.)? Regression, which is the main topic in this course, can be seen as a measurement error model. Under these models, we are interested in learning about some underlying pattern. One simple example is linear regression with one predictor with data measured with error, i.e., \\(y_i=a+bx_i+\\epsilon_i\\). 2.1 Sampling distribution Sampling distribution may be considered as the distribution of the statistic for all possible samples from the same population of a given sample size. - Random sampling. Simple random sample of size \\(n\\) from a population of size \\(N\\). - Measurement error model. Observations \\(y_i\\) generated from the model \\(y_i=a+bx_i+\\epsilon_i\\) with a prespecified distribution for the errors, e.g., \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma)\\) Most of the time the sampling distribution will not be known, therefore we can only estimate it. Let’s see this using the following example using the previous measurment error model \\(y_i=a+bx_i+\\epsilon_i\\). We set up \\[ a=3,\\space b=2,\\space \\epsilon_i \\sim \\mathcal{N}(0,1),\\space x_i\\sim\\mathcal{N}(0,2); \\space i=1,...,100 \\] a=3 b=2 n=100 epsilon=rnorm(n,0,1) x=rnorm(n,0,2) y=a+b*x+epsilon Now let’s fit a linear model to our fake data. Table 2.1 shows the 95% CI for the fitted slope and intercept. model=lm(y~x) summary(model) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.05227 -0.67541 0.03454 0.63157 1.93196 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.16148 0.09389 33.67 &lt;2e-16 *** ## x 2.07163 0.04853 42.69 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9303 on 98 degrees of freedom ## Multiple R-squared: 0.949, Adjusted R-squared: 0.9485 ## F-statistic: 1823 on 1 and 98 DF, p-value: &lt; 2.2e-16 Table 2.1: 95% CI for intercept and slope 2.5 % 97.5 % (Intercept) 2.975154 3.347808 x 1.975334 2.167931 Some authors say that the sampling distribution is a generative model because it represents a random process wich could potentially generate new data. 2.2 Estimates and standard erors Parameters are unknown numbers that determine a statistical model. As mentioned before, we learn from data to construct estimates of parameters. Question What are the parameters in the measurement error model used previously? The standard error is the standard deviation of an estimate. It gives us an idea of the uncertainty sorrounding the quantity of interest. As sample size increases, the standard error decreases, i.e., \\[ SE=\\frac{\\sigma}{n} \\implies \\lim_{n\\to\\infty}\\frac{\\sigma}{n}=0 \\] "],["linear-regression.html", "Chapter 3 Linear regression 3.1 Background 3.2 Simple regression with fake data 3.3 Interpretation of coefficients and model diagnostics", " Chapter 3 Linear regression 3.1 Background Simplest regression model: \\(y=a+bx+error\\) Additional predictors: : \\(y=\\beta_0+\\beta_1x_1+...+\\beta_kx_k+error\\) Nonlinear models: \\(\\log y=a+b \\log x+error\\) Nonadditive models: \\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2+error\\) Generalized linear models: Non-normal additive errors with a function that “links” the linear predictor with the outcome. 3.2 Simple regression with fake data Same as Chapter 2 x=1:20 n=length(x) a=0.2 b=0.3 sigma=0.5 y=a + b*x + sigma*rnorm(n) fit=lm(y~x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.34282 -0.13563 0.06904 0.25874 0.40137 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.19986 0.18555 -1.077 0.296 ## x 0.34434 0.01549 22.231 1.54e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3994 on 18 degrees of freedom ## Multiple R-squared: 0.9649, Adjusted R-squared: 0.9629 ## F-statistic: 494.2 on 1 and 18 DF, p-value: 1.541e-14 Table 3.1: 95% CI for intercept and slope 2.5 % 97.5 % (Intercept) -0.5896780 0.1899658 x 0.3117944 0.3768778 Questions Are estimates consistent with true values? What can you tell about the uncertainty surrounding the coefficients? We can also plot the data and fitter regression line (Figure 3.1). plot(x, y, main=&quot;Data and fitted regression line&quot;) a_hat=coef(fit)[1] b_hat=coef(fit)[2] abline(a_hat, b_hat) Figure 3.1: Observed and fitted 3.3 Interpretation of coefficients and model diagnostics Linear regression assumptions Normality of residuals. Linear relationship. Constant variance. \\(y_i\\) are independent. Let’s start using real data. Below we can see a brief description about the dataset used in this example. install.packages(&quot;datarium&quot;) # Install to access the dataset ?marketing #description of the dataset library(datarium) head(marketing) youtube facebook newspaper sales 276.12 45.36 83.04 26.52 53.40 47.16 54.12 12.48 20.64 55.08 83.16 11.16 181.80 49.56 70.20 22.20 216.96 12.96 70.08 15.48 10.44 58.68 90.00 8.64 We build a model to predict sales on the basis of advertising budget spent in youtube and facebook. It is important to notice the nonadditive model used here and the implications of interaction terms. fit2=lm(sales ~ youtube*facebook, data = marketing) summary(fit2) ## ## Call: ## lm(formula = sales ~ youtube * facebook, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.6039 -0.4833 0.2197 0.7137 1.8295 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.100e+00 2.974e-01 27.233 &lt;2e-16 *** ## youtube 1.910e-02 1.504e-03 12.699 &lt;2e-16 *** ## facebook 2.886e-02 8.905e-03 3.241 0.0014 ** ## youtube:facebook 9.054e-04 4.368e-05 20.727 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.132 on 196 degrees of freedom ## Multiple R-squared: 0.9678, Adjusted R-squared: 0.9673 ## F-statistic: 1963 on 3 and 196 DF, p-value: &lt; 2.2e-16 Also we plot some regression diagnostics. plot(fit2) "],["references.html", "References", " References "]]
