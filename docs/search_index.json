[["index.html", "Tutorial notes for Biostatistics II Chapter 1 Common pitfalls in statistical analysis 1.1 p-values 1.2 Table 1 1.3 Variables", " Tutorial notes for Biostatistics II Juan Pablo Diaz Martinez 2024-01-18 Chapter 1 Common pitfalls in statistical analysis 1.1 p-values The p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct. Example Suppose we want to compare fasting serum-cholesterol levels among recent Asian immigrants to the United States with typical levels found in the general U.S. population. Suppose we assume cholesterol levels in women ages 21−40 in the United States are approximately normally distributed with mean 190 mg/dL. It is unknown whether cholesterol levels among recent Asian immigrants are higher or lower than those in the general U.S. population. Let’s assume that levels among recent female Asian immigrants are normally distributed with unknown mean \\(\\mu\\). Hence we wish to test \\[ \\begin{aligned} H_0 &amp;: \\mu=\\mu_0 \\\\ H_1 &amp;: \\mu \\neq \\mu_0 \\end{aligned} \\] Blod tests are performed on 100 female Asian immigrants ages 21−40, and the mean level (x) is 181.52 mg/dL with standard error = 40 mg/dL. What can we conclude on the basis of this evidence? First, the test statistic would be: \\[ \\begin{aligned} t &amp;= (\\bar{x}-\\mu_0)/(s/\\sqrt{n}) \\\\ t &amp;= -8.48/4 = -2.12 \\end{aligned} \\] Under the null, this test statistic follows a t-distribution with \\(n-1\\) degrees of freedom, i.e, \\[ t \\sim t(n-1) \\] Mathematically, for symmetric distributions (such as the t-distribution), the p-value for two-sided tests is define as: \\[ \\begin{aligned} p &amp;=2*\\Pr[|T| \\geq |t| \\space | \\space H_0] \\\\ p &amp;=2*\\Pr[|T| \\geq 2.12 \\space | \\space H_0]=0.37 \\end{aligned} \\] where \\(T\\) is the distribution under the null and \\(t\\) 2*pt(2.12,df=99,lower.tail = F) ## [1] 0.03650607 Missconceptions behind p-values The p-value is not the probability that the null hypothesis is true, or the probability that the alternative hypothesis is false The p-value is not the probability that the observed effects were produced by random chance alone The 0.05 significance level is merely a convention The p-value does not indicate the size or importance of the observed effect 1.2 Table 1 It is recommended to not report p-values when comparing summary statistics between two or more groups because: Table 1 usually describes the study sample Valid p-values cannot be drawn without knowing, not just what was done with the existing data, but what the choices in data coding, exclusion, and analysis would have been, had the data been different Being significant does not imply anything in a regression context One recommendation is to report the standardized mean difference (SMD). The p-values are a function of a sample size, meaning that with a large sample most likelu we will find a significal difference. 1.3 Variables Do not test for normality. Majority of parametric tests assumptions are based on the mean or the residuals (linear regression model) Report missing values "],["intro.html", "Chapter 2 Statistical inference 2.1 Sampling distribution 2.2 Estimates and standard erors", " Chapter 2 Statistical inference These notes are based on “Regression and other stories” by Gelman, Hill, and Vehtari (2020). Given the different background in statistics that we potentially have, this notes aim to have a standardized tool for tutorials. Finally, these notes aim to cover the same content as weekly lectures. Statistical inference involves math operations that result in estimates (and its uncertainties) about paramaters of some underlying process. We can think it as a learning process; we learn from data (incomplete or imperfect) with the objective of informing the parameters under a probability model. Questions What type of probability model is involved in linear regression? What about Fisher’s exact test? What about nonparametric tests (Mann-Whitney etc.)? Regression, which is the main topic in this course, can be seen as a measurement error model. Under these models, we are interested in learning about some underlying pattern. One simple example is linear regression with one predictor with data measured with error, i.e., \\(y_i=a+bx_i+\\epsilon_i\\). 2.1 Sampling distribution Sampling distribution may be considered as the distribution of the statistic for all possible samples from the same population of a given sample size. Random sampling. Simple random sample of size \\(n\\) from a population of size \\(N\\). Measurement error model. Observations \\(y_i\\) generated from the model \\(y_i=a+bx_i+\\epsilon_i\\) with a prespecified distribution for the errors, e.g., \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma)\\) Most of the time the sampling distribution will not be known, therefore we can only estimate it. Let’s see this using the following example using the previous measurment error model \\(y_i=a+bx_i+\\epsilon_i\\). We set up \\[ a=3,\\space b=2,\\space \\epsilon_i \\sim \\mathcal{N}(0,1),\\space x_i\\sim\\mathcal{N}(0,2); \\space i=1,...,100 \\] a=3 b=2 n=100 epsilon=rnorm(n,0,1) x=rnorm(n,0,2) y=a+b*x+epsilon Now let’s fit a linear model to our fake data. Table 2.1 shows the 95% CI for the fitted slope and intercept. model=lm(y~x) summary(model) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.23995 -0.68303 0.06612 0.56030 2.02661 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.00686 0.09414 31.94 &lt;2e-16 *** ## x 2.00346 0.04598 43.57 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9312 on 98 degrees of freedom ## Multiple R-squared: 0.9509, Adjusted R-squared: 0.9504 ## F-statistic: 1898 on 1 and 98 DF, p-value: &lt; 2.2e-16 Table 2.1: 95% CI for intercept and slope 2.5 % 97.5 % (Intercept) 2.820047 3.193666 x 1.912209 2.094713 Some authors say that the sampling distribution is a generative model because it represents a random process wich could potentially generate new data. 2.2 Estimates and standard erors Parameters are unknown numbers that determine a statistical model. As mentioned before, we learn from data to construct estimates of parameters. Question What are the parameters in the measurement error model used previously? The standard error is the standard deviation of an estimate. It gives us an idea of the uncertainty sorrounding the quantity of interest. As sample size increases, the standard error decreases, i.e., \\[ SE=\\frac{\\sigma}{n} \\implies \\lim_{n\\to\\infty}\\frac{\\sigma}{n}=0 \\] References "],["linear-regression.html", "Chapter 3 Linear regression 3.1 Background 3.2 Simple regression with fake data 3.3 Interpretation of coefficients and model diagnostics 3.4 Outliers and how to diagnose them", " Chapter 3 Linear regression 3.1 Background Simplest regression model: \\(y=a+bx+error\\) Additional predictors: : \\(y=\\beta_0+\\beta_1x_1+...+\\beta_kx_k+error\\) Nonlinear models: \\(\\log y=a+b \\log x+error\\) Nonadditive models: \\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2+error\\) Generalized linear models: Non-normal additive errors with a function that “links” the linear predictor with the outcome. 3.2 Simple regression with fake data Same as Chapter 2 x=1:20 n=length(x) a=0.2 b=0.3 sigma=0.5 y=a + b*x + sigma*rnorm(n) fit=lm(y~x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.46189 -0.19521 0.05332 0.33945 0.90045 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.07808 0.28443 0.275 0.787 ## x 0.30694 0.02374 12.927 1.51e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6123 on 18 degrees of freedom ## Multiple R-squared: 0.9028, Adjusted R-squared: 0.8974 ## F-statistic: 167.1 on 1 and 18 DF, p-value: 1.509e-10 Table 3.1: 95% CI for intercept and slope 2.5 % 97.5 % (Intercept) -0.5194814 0.6756454 x 0.2570520 0.3568192 Questions Are estimates consistent with true values? What can you tell about the uncertainty surrounding the coefficients? We can also plot the data and fitter regression line (Figure 3.1). plot(x, y, main=&quot;Data and fitted regression line&quot;) a_hat=coef(fit)[1] b_hat=coef(fit)[2] abline(a_hat, b_hat) Figure 3.1: Observed and fitted 3.3 Interpretation of coefficients and model diagnostics Linear regression assumptions Normality of residuals. Linear relationship. Constant variance. \\(y_i\\) are independent. Let’s start using real data. Below we can see a brief description about the dataset used in this example. install.packages(&quot;datarium&quot;) # Install to access the dataset ?marketing #description of the dataset library(datarium) head(marketing) youtube facebook newspaper sales 276.12 45.36 83.04 26.52 53.40 47.16 54.12 12.48 20.64 55.08 83.16 11.16 181.80 49.56 70.20 22.20 216.96 12.96 70.08 15.48 10.44 58.68 90.00 8.64 We build a model to predict sales on the basis of advertising budget spent in youtube and facebook. It is important to notice the nonadditive model used here and the implications of interaction terms. fit2=lm(sales ~ youtube*facebook, data = marketing) summary(fit2) ## ## Call: ## lm(formula = sales ~ youtube * facebook, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.6039 -0.4833 0.2197 0.7137 1.8295 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.100e+00 2.974e-01 27.233 &lt;2e-16 *** ## youtube 1.910e-02 1.504e-03 12.699 &lt;2e-16 *** ## facebook 2.886e-02 8.905e-03 3.241 0.0014 ** ## youtube:facebook 9.054e-04 4.368e-05 20.727 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.132 on 196 degrees of freedom ## Multiple R-squared: 0.9678, Adjusted R-squared: 0.9673 ## F-statistic: 1963 on 3 and 196 DF, p-value: &lt; 2.2e-16 Also we plot some regression diagnostics. plot(fit2) 3.4 Outliers and how to diagnose them Let’s recall our linear model, i.e., \\(y_i=a+bx_i+\\epsilon_i,\\quad \\epsilon_i\\sim \\mathcal{N}(0,\\sigma)\\). Why is important to check the outliers after fitting our model? Because sometimes we can encounter potential outliers that could influence the regression model in the sense of pulling our fitted model towards these outliers. 3.4.1 Studentized residuals After fitting our linear model, we get \\[ y_i=\\hat{a}+\\hat{b}x_i+d_i \\implies d_i=y_i-\\hat{y_i} \\] where \\(d_i\\) denotes the residual for observation \\(i\\) and \\(\\hat{y_i}=\\hat{a}+\\hat{b}x_i\\). Let’s plot again our fitted values vs residuals in our previous model. How large is large? plot(fit2,1) In order to answer the previous question, we standardize our residuals, i.e., \\[ t_i=\\frac{d_i}{sd(d_i)} \\] This studentized residuals follow a student’s t distribution, i.e., \\(t_i\\sim t_{n-p-1}\\) where \\(p\\) is the number of coefficients in our regression model. Questions Why do residuals \\(&gt;2\\) warrant attention? 3.4.2 Difference in Fits (DFFITS) Are outliers bad when fitting a linear model? Not necessarily. We can have outliers which are not influential in our fitted model. The idea behind diagnosing influential observations is to delete the observations one at a time, each time refitting the regression model on the remaining \\(n–1\\) observations. The difference in fits for observation \\(i\\), denoted \\(DFFITS_i\\), is defined as: \\[ DFFITS_i=\\frac{\\hat{y_i}-\\hat{y}_{(i)}}{sd(\\hat{y_i}-\\hat{y}_{(i)})} \\] where \\(\\hat{y}_{(i)}\\) denotes the fitted response when observation \\(i\\) is removed. Going back to our previous example, Figure 3.2 shows how influential our observations can be plot(dffits(fit2)) Figure 3.2: DFFITS \\(DFFITS_i\\) can also be expressed as \\[ DFFITS_i=t_{(i)}\\sqrt{\\frac{h_{ii}}{1-h_{ii}}} \\] where \\(t_{(i)}\\) is the externally studentized residual and \\(h_ii\\) represents the leverage. The leverage \\(h_ii\\) is a measure of the distance between the \\(x\\) value for the \\(i\\)th data point and the mean of the \\(x\\) values for all \\(n\\) data points. For a perfectly balanced experimental design, the leverage for each point is \\(p/n\\), therefore \\[ DFFITS_i=t_{(i)}\\sqrt{\\frac{p}{n}} \\] "],["logistic-regression.html", "Chapter 4 Logistic regression 4.1 Interpretation of coefficients and model diagnostics", " Chapter 4 Logistic regression Recall the linear model \\(y=a+bx+error\\). To model binary data, it is clear that \\(a+bx\\) is unbounded. Therefore we need some kind of transformation that bounds the output between 0 and 1, and a model that treats the resulting numbers as probabilities and maps them into random binary outcomes. The logistic function, \\[ logit(x)=\\log \\left (\\frac{x}{1-x}\\right) \\] which maps the range (0,1) to the real line \\((-\\infty,\\infty)\\). Its inverse function, maps back to the unit range \\[ logit^{-1}(x)=\\frac{\\exp{(x)}}{1+\\exp{(x)}} \\] The idea of logistic regression is to model the probability that \\(y=1\\) as \\[ \\begin{aligned} \\mathbb{P}[y_i=1] &amp;= p_i \\\\ logit(p_i) &amp;= X_i\\beta \\end{aligned} \\] 4.1 Interpretation of coefficients and model diagnostics Logistic regression diagnostics Model fit. No influential observations. First we need to load the dataset to be used in this chapter. df=read.csv(&quot;SAS Jupyter/tutorial2.csv&quot;) kable(head(df), booktabs = TRUE) Treatment Sex Age Duration Pain P F 68 1 No B M 74 16 No P F 67 30 No P M 66 26 Yes B F 67 28 No B F 77 16 No df$Treatment=as.factor(df$Treatment) df$Pain=as.factor(df$Pain) df$Sex=as.factor(df$Sex) Same as our previous chapter, we build a model to investigate the relationship between pain and other covariates. fit3=glm(Pain ~ Treatment+Sex+Treatment*Sex+Age+Duration, data = df,family = binomial) summary(fit3) ## ## Call: ## glm(formula = Pain ~ Treatment + Sex + Treatment * Sex + Age + ## Duration, family = binomial, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8542 -0.6273 -0.1993 0.6037 2.2047 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -20.78814 7.18282 -2.894 0.00380 ** ## TreatmentB -0.89641 1.64483 -0.545 0.58576 ## TreatmentP 2.83808 1.32379 2.144 0.03204 * ## SexM 1.43252 1.33133 1.076 0.28192 ## Age 0.26875 0.09964 2.697 0.00699 ** ## Duration -0.00523 0.03330 -0.157 0.87518 ## TreatmentB:SexM 0.49954 1.92350 0.260 0.79509 ## TreatmentP:SexM 0.70670 1.93771 0.365 0.71533 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 81.503 on 59 degrees of freedom ## Residual deviance: 48.596 on 52 degrees of freedom ## AIC: 64.596 ## ## Number of Fisher Scoring iterations: 6 By default, estimates in R are shown as log-odds. If we want to get the odd ratios (ORs) we just need to exponentiate them (applies also to confidence intervals) ORs=exp(cbind(OR = coef(fit3), confint(fit3))) kable(ORs, booktabs = TRUE) OR 2.5 % 97.5 % (Intercept) 0.0000000 0.0000000 0.0002309 TreatmentB 0.4080301 0.0113346 13.9284003 TreatmentP 17.0830112 1.6920908 434.6827048 SexM 4.1892546 0.3700386 104.5704058 Age 1.3083321 1.1009612 1.6382785 Duration 0.9947833 0.9293463 1.0610565 TreatmentB:SexM 1.6479605 0.0311099 93.0296128 TreatmentP:SexM 2.0272960 0.0381990 121.1555591 Questions Whay an you say about the CIs for the treatment variable? We can now check our model fit with the Hosmer-Lemeshow test. install.packages(&quot;ResourceSelection&quot;) # Install to access functions library(ResourceSelection) hoslem.test(fit3$y, fitted(fit3), g=10) ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: fit3$y, fitted(fit3) ## X-squared = 9.3995, df = 8, p-value = 0.3097 "],["generalized-linear-models.html", "Chapter 5 Generalized linear models 5.1 Introduction 5.2 Poisson regression 5.3 Negative Binomial regression model 5.4 Example: pest management on Roaches reducing cockroach level", " Chapter 5 Generalized linear models 5.1 Introduction Generalized linear modeling (GLM) is a framework which encompasses linear and logistic regression as special cases. It involves: An outcome \\(y=(y_1,\\dots,y_n)\\) A matrix of predictors \\(X\\) and a vector of coefficients \\(\\beta\\), which turns into the linear predictor \\(X\\beta\\) A link function \\(g\\), which maps the linear predictor with the outcome \\(\\hat{y}=g^{-1}(X\\beta)\\) A data distribution \\(p(y|\\hat{y})\\) Other parameters involved either in 2,3 or 4. Linear regression: \\(g(u)\\equiv u\\) and data distribution normal Logistic regression: \\(g^{-1}(u)=\\text{logit}^{-1}(u)\\) and data distribution defined by \\(\\mathbb{P}[y=1]=\\hat{y}\\) 5.2 Poisson regression In count-data regressions, each unit i corresponds to a setting (typically a spatial location or a time-interval) in which \\(y_i\\) events are observed. The model that generates the data in a Poisson regression model is: \\[\\begin{align*} y_i &amp;\\sim \\text{Poisson}(\\lambda_i) \\\\ \\lambda_i &amp;=\\exp(X_i\\beta) \\end{align*}\\] Questions Why not using linear regression directly? What is the link function for the Poisson regression model? What is the mean and the variance of a Poisson random variable? As always, let’s fake data and fit the model to our simulated data: n=50 x=runif(n, -2, 2) a=1 b=2 linpred=a + b*x y=rpois(n, exp(linpred)) fake=data.frame(x=x, y=y) fit3=glm(y ~ x, data = fake,family = poisson) summary(fit3) ## ## Call: ## glm(formula = y ~ x, family = poisson, data = fake) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1412 -0.7044 -0.3791 0.2430 1.8865 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.07608 0.10867 9.902 &lt;2e-16 *** ## x 1.99111 0.06732 29.579 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 2422.853 on 49 degrees of freedom ## Residual deviance: 37.954 on 48 degrees of freedom ## AIC: 188.99 ## ## Number of Fisher Scoring iterations: 4 plot(x, y) curve(exp(coef(fit3)[1] + coef(fit3)[2]*x), add=TRUE) Figure 5.1: Observed and fitted 5.2.1 Exposure Most applications of count-data regression involve an exposure. A simple example can be the number of adverse events in a period of time. It is logical to expect a greater number of these events for those patients followed for a longer period of time. We can include this exposure \\(u_i\\) as \\[ \\begin{align*} y_i &amp;\\sim \\text{Poisson}(\\lambda_i) \\\\ \\lambda_i &amp;= u_i\\exp(X_i\\beta) \\end{align*} \\] 5.3 Negative Binomial regression model Overdispersion and underdispersion refer to data that show more or less variation than expected based on a fitted probability model. The negative binomial regression model which accounts for dispersion. In this model, \\(\\phi\\) represents “reciprocal dispersion” parameter so that \\(sd(y|x)=\\sqrt{\\mathbb{E}(y|x)+\\mathbb{E}(y|x)^2/\\phi}\\). Therefore \\[ \\begin{align*} y_i &amp;\\sim \\text{NB}(\\lambda_i,\\phi) \\\\ \\lambda_i &amp;=\\exp(X_i\\beta) \\end{align*} \\] Exposure can be incorporated in negative binomial models 5.4 Example: pest management on Roaches reducing cockroach level We will use the roaches dataset from rstanarm in R, which study the effect of a treatment and control in pest control. The outcome \\(y_i\\) in each apartment \\(i\\) was the number of roaches caught in a set of traps. Different apartments had traps for different numbers of days. We fitted a negative binomial and Poisson model to our data. library(MASS) roaches=rstanarm::roaches roaches$roach100=roaches$roach1/100 fit4=glm.nb(y ~ roach100 + treatment + senior + offset(log(exposure2)), data=roaches) summary(fit4) ## ## Call: ## glm.nb(formula = y ~ roach100 + treatment + senior + offset(log(exposure2)), ## data = roaches, init.theta = 0.2742646256, link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8014 -1.3282 -0.6850 -0.0218 3.2021 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.8388 0.2134 13.301 &lt; 2e-16 *** ## roach100 1.2882 0.1590 8.102 5.43e-16 *** ## treatment -0.7768 0.2445 -3.177 0.00149 ** ## senior -0.3444 0.2631 -1.309 0.19059 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(0.2743) family taken to be 1) ## ## Null deviance: 344.06 on 261 degrees of freedom ## Residual deviance: 277.78 on 258 degrees of freedom ## AIC: 1788.6 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 0.2743 ## Std. Err.: 0.0262 ## ## 2 x log-likelihood: -1778.5830 fit5=glm(y ~ roach100 + treatment + senior + offset(log(exposure2)),family=poisson, data=roaches) summary(fit5) ## ## Call: ## glm(formula = y ~ roach100 + treatment + senior + offset(log(exposure2)), ## family = poisson, data = roaches) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -17.9430 -5.1529 -3.8059 0.1452 26.7771 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.089246 0.021234 145.49 &lt;2e-16 *** ## roach100 0.698289 0.008874 78.69 &lt;2e-16 *** ## treatment -0.516726 0.024739 -20.89 &lt;2e-16 *** ## senior -0.379875 0.033418 -11.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 16954 on 261 degrees of freedom ## Residual deviance: 11429 on 258 degrees of freedom ## AIC: 12192 ## ## Number of Fisher Scoring iterations: 6 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
