[["index.html", "Tutorial notes for Biostatistics II Chapter 1 Introduction", " Tutorial notes for Biostatistics II Juan Pablo Diaz Martinez 2021-01-22 Chapter 1 Introduction These notes are based on “Regression and other stories” by Gelman, Hill, and Vehtari (2020). Given the different background in statistics that we potentially have, this notes aim to have a standardized tool for tutorials. Finally, these notes aim to cover the same content as weekly lectures. References "],["intro.html", "Chapter 2 Statistical inference 2.1 Sampling distribution 2.2 Estimates and standard erors", " Chapter 2 Statistical inference Statistical inference involves math operations that result in estimates (and its uncertainties) about paramaters of some underlying process. We can think it as a learning process; we learn from data (incomplete or imperfect) with the objective of informing the parameters under a probability model. Questions What type of probability model is involved in linear regression? What about Fisher’s exact test? What about nonparametric tests (Mann-Whitney etc.)? Regression, which is the main topic in this course, can be seen as a measurement error model. Under these models, we are interested in learning about some underlying pattern. One simple example is linear regression with one predictor with data measured with error, i.e., \\(y_i=a+bx_i+\\epsilon_i\\). 2.1 Sampling distribution Sampling distribution may be considered as the distribution of the statistic for all possible samples from the same population of a given sample size. Random sampling. Simple random sample of size \\(n\\) from a population of size \\(N\\). Measurement error model. Observations \\(y_i\\) generated from the model \\(y_i=a+bx_i+\\epsilon_i\\) with a prespecified distribution for the errors, e.g., \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma)\\) Most of the time the sampling distribution will not be known, therefore we can only estimate it. Let’s see this using the following example using the previous measurment error model \\(y_i=a+bx_i+\\epsilon_i\\). We set up \\[ a=3,\\space b=2,\\space \\epsilon_i \\sim \\mathcal{N}(0,1),\\space x_i\\sim\\mathcal{N}(0,2); \\space i=1,...,100 \\] a=3 b=2 n=100 epsilon=rnorm(n,0,1) x=rnorm(n,0,2) y=a+b*x+epsilon Now let’s fit a linear model to our fake data. Table 2.1 shows the 95% CI for the fitted slope and intercept. model=lm(y~x) summary(model) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.62562 -0.50542 0.01512 0.53543 2.44121 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.99598 0.08386 35.73 &lt;2e-16 *** ## x 1.93193 0.04105 47.07 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8353 on 98 degrees of freedom ## Multiple R-squared: 0.9576, Adjusted R-squared: 0.9572 ## F-statistic: 2215 on 1 and 98 DF, p-value: &lt; 2.2e-16 Table 2.1: 95% CI for intercept and slope 2.5 % 97.5 % (Intercept) 2.829560 3.162395 x 1.850468 2.013383 Some authors say that the sampling distribution is a generative model because it represents a random process wich could potentially generate new data. 2.2 Estimates and standard erors Parameters are unknown numbers that determine a statistical model. As mentioned before, we learn from data to construct estimates of parameters. Question What are the parameters in the measurement error model used previously? The standard error is the standard deviation of an estimate. It gives us an idea of the uncertainty sorrounding the quantity of interest. As sample size increases, the standard error decreases, i.e., \\[ SE=\\frac{\\sigma}{n} \\implies \\lim_{n\\to\\infty}\\frac{\\sigma}{n}=0 \\] "],["linear-regression.html", "Chapter 3 Linear regression 3.1 Background 3.2 Simple regression with fake data 3.3 Interpretation of coefficients and model diagnostics 3.4 Outliers and how to diagnose them", " Chapter 3 Linear regression 3.1 Background Simplest regression model: \\(y=a+bx+error\\) Additional predictors: : \\(y=\\beta_0+\\beta_1x_1+...+\\beta_kx_k+error\\) Nonlinear models: \\(\\log y=a+b \\log x+error\\) Nonadditive models: \\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2+error\\) Generalized linear models: Non-normal additive errors with a function that “links” the linear predictor with the outcome. 3.2 Simple regression with fake data Same as Chapter 2 x=1:20 n=length(x) a=0.2 b=0.3 sigma=0.5 y=a + b*x + sigma*rnorm(n) fit=lm(y~x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.67613 -0.39121 0.08526 0.22592 0.77421 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.06941 0.20653 0.336 0.741 ## x 0.30489 0.01724 17.685 7.97e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4446 on 18 degrees of freedom ## Multiple R-squared: 0.9456, Adjusted R-squared: 0.9426 ## F-statistic: 312.7 on 1 and 18 DF, p-value: 7.966e-13 Table 3.1: 95% CI for intercept and slope 2.5 % 97.5 % (Intercept) -0.3644844 0.5033083 x 0.2686696 0.3411115 Questions Are estimates consistent with true values? What can you tell about the uncertainty surrounding the coefficients? We can also plot the data and fitter regression line (Figure 3.1). plot(x, y, main=&quot;Data and fitted regression line&quot;) a_hat=coef(fit)[1] b_hat=coef(fit)[2] abline(a_hat, b_hat) Figure 3.1: Observed and fitted 3.3 Interpretation of coefficients and model diagnostics Linear regression assumptions Normality of residuals. Linear relationship. Constant variance. \\(y_i\\) are independent. Let’s start using real data. Below we can see a brief description about the dataset used in this example. install.packages(&quot;datarium&quot;) # Install to access the dataset ?marketing #description of the dataset library(datarium) head(marketing) youtube facebook newspaper sales 276.12 45.36 83.04 26.52 53.40 47.16 54.12 12.48 20.64 55.08 83.16 11.16 181.80 49.56 70.20 22.20 216.96 12.96 70.08 15.48 10.44 58.68 90.00 8.64 We build a model to predict sales on the basis of advertising budget spent in youtube and facebook. It is important to notice the nonadditive model used here and the implications of interaction terms. fit2=lm(sales ~ youtube*facebook, data = marketing) summary(fit2) ## ## Call: ## lm(formula = sales ~ youtube * facebook, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.6039 -0.4833 0.2197 0.7137 1.8295 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.100e+00 2.974e-01 27.233 &lt;2e-16 *** ## youtube 1.910e-02 1.504e-03 12.699 &lt;2e-16 *** ## facebook 2.886e-02 8.905e-03 3.241 0.0014 ** ## youtube:facebook 9.054e-04 4.368e-05 20.727 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.132 on 196 degrees of freedom ## Multiple R-squared: 0.9678, Adjusted R-squared: 0.9673 ## F-statistic: 1963 on 3 and 196 DF, p-value: &lt; 2.2e-16 Also we plot some regression diagnostics. plot(fit2) 3.4 Outliers and how to diagnose them Let’s recall our linear model, i.e., \\(y_i=a+bx_i+\\epsilon_i\\space \\epsilon_i\\sim \\mathcal{N}(0,\\sigma)\\). Why is important to check the outliers after fitting our model? Because sometimes we can encounter potential outliers that could influence the regression model in the sense of pulling our fitted model towards these outliers. 3.4.1 Studentized resiudals After fitting our linear model, we get \\[ y_i=\\hat{a}+\\hat{b}x_i+d_i \\implies d_i=y_i-\\hat{y_i} \\] where \\(d_i\\) denotes the residual for observation \\(i\\) and \\(\\hat{y_i}=\\hat{a}+\\hat{b}x_i\\). Let’s plot again our fitted values vs residuals in our previous model. How large is large? plot(fit2,1) In order to answer the previous question, we standardize our residuals, i.e., \\[ t_i=\\frac{d_i}{sd(d_i)} \\] This studentized residuals follow a student’s t distribution, i.e., \\(t_i\\sim t_{n-p-1}\\) where \\(p\\) is the number of coefficients in our regression model. Questions Why do residuals \\(&gt;2\\) warrant attention? 3.4.2 Difference in Fits (DFFITS) Are outliers bad when fitting a linear model? Not necessarily. We can have outliers which are not influential in our fitted model. The idea behind diagnosing influential observations is to delete the observations one at a time, each time refitting the regression model on the remaining \\(n–1\\) observations. The difference in fits for observation \\(i\\), denoted \\(DFFITS_i\\), is defined as: \\[ DFFITS_i=\\frac{\\hat{y_i}-\\hat{y}_{(i)}}{sd(\\hat{y_i}-\\hat{y}_{(i)})} \\] where \\(\\hat{y}_{(i)}\\) denotes the fitted response when observation \\(i\\) is removed. Going back to our previous example, Figure 3.2 shows how influential our observations can be plot(dffits(fit2)) Figure 3.2: DFFITS \\(DFFITS_i\\) can also be expressed as \\[ DFFITS_i=t_{(i)}\\sqrt{\\frac{h_{ii}}{1-h_{ii}}} \\] where \\(t_{(i)}\\) is the externally studentized residual and \\(h_ii\\) represents the leverage. The leverage \\(h_ii\\) is a measure of the distance between the \\(x\\) value for the \\(i\\)th data point and the mean of the \\(x\\) values for all \\(n\\) data points. For a perfectly balanced experimental design, the leverage for each point is \\(p/n\\), therefore \\[ DFFITS_i=t_{(i)}\\sqrt{\\frac{p}{n}} \\] "],["logistic-regression.html", "Chapter 4 Logistic regression 4.1 Interpretation of coefficients and model diagnostics", " Chapter 4 Logistic regression Recall the linear model \\(y=a+bx+error\\). To model binary data, it is clear that \\(a+bx\\) is unbounded. Therefore we need some kind of transformation that bounds the output between 0 and 1, and a model that treats the resulting numbers as probabilities and maps them into random binary outcomes. The logistic function, \\[ logit(x)=\\log \\left (\\frac{x}{1-x}\\right) \\] which maps the range (0,1) to the real line \\((-\\infty,\\infty)\\). Its inverse function, maps back to the unit range \\[ logit^{-1}(x)=\\frac{\\exp{(x)}}{1+\\exp{(x)}} \\] The idea of logistic regression is to model the probability that \\(y=1\\) as \\[ \\begin{aligned} \\mathbb{P}[y_i=1] &amp;= p_i \\\\ logit(p_i) &amp;= X_i\\beta \\end{aligned} \\] ## Assumptions 4.1 Interpretation of coefficients and model diagnostics Logistic regression diagnostics Model fit. No influential observations. First we need to load the dataset to be used in this chapter. df=read.csv(&quot;data/tutorial2.csv&quot;) kable(head(df), booktabs = TRUE) Treatment Sex Age Duration Pain P F 68 1 No B M 74 16 No P F 67 30 No P M 66 26 Yes B F 67 28 No B F 77 16 No df$Treatment=as.factor(df$Treatment) df$Pain=as.factor(df$Pain) df$Sex=as.factor(df$Sex) Same as our previous chapter, we build a model to investigate the relationship between pain and other covariates. fit3=glm(Pain ~ Treatment+Sex+Treatment*Sex+Age+Duration, data = df,family = binomial) summary(fit3) ## ## Call: ## glm(formula = Pain ~ Treatment + Sex + Treatment * Sex + Age + ## Duration, family = binomial, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8542 -0.6273 -0.1993 0.6037 2.2047 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -20.78814 7.18282 -2.894 0.00380 ** ## TreatmentB -0.89641 1.64483 -0.545 0.58576 ## TreatmentP 2.83808 1.32379 2.144 0.03204 * ## SexM 1.43252 1.33133 1.076 0.28192 ## Age 0.26875 0.09964 2.697 0.00699 ** ## Duration -0.00523 0.03330 -0.157 0.87518 ## TreatmentB:SexM 0.49954 1.92350 0.260 0.79509 ## TreatmentP:SexM 0.70670 1.93771 0.365 0.71533 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 81.503 on 59 degrees of freedom ## Residual deviance: 48.596 on 52 degrees of freedom ## AIC: 64.596 ## ## Number of Fisher Scoring iterations: 6 By default, estimates in R are shown as log-odds. If we want to get the odd ratios (ORs) we just need to exponentiate them (applies also to confidence intervals) ORs=exp(cbind(OR = coef(fit3), confint(fit3))) kable(ORs, booktabs = TRUE) OR 2.5 % 97.5 % (Intercept) 0.0000000 0.0000000 0.0002309 TreatmentB 0.4080301 0.0113346 13.9284003 TreatmentP 17.0830112 1.6920908 434.6827048 SexM 4.1892546 0.3700386 104.5704058 Age 1.3083321 1.1009612 1.6382785 Duration 0.9947833 0.9293463 1.0610565 TreatmentB:SexM 1.6479605 0.0311099 93.0296128 TreatmentP:SexM 2.0272960 0.0381990 121.1555591 Questions Whay an you say about the CIs for the treatment variable? We can now check our model fit with the Hosmer-Lemeshow test. install.packages(&quot;ResourceSelection&quot;) # Install to access functions library(ResourceSelection) hoslem.test(fit3$y, fitted(fit3), g=10) ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: fit3$y, fitted(fit3) ## X-squared = 9.3995, df = 8, p-value = 0.3097 "],["references.html", "References", " References "]]
